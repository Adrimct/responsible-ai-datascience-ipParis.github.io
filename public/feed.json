{
  "version": "https://jsonfeed.org/version/1",
  "title": "Bloggin on Responsible AI",
  "home_page_url": "http://localhost:1313/",
  "feed_url": "http://localhost:1313/feed.json",
  "description": "Bloggin on Responsible AI",
  "favicon": "http://localhost:1313//assets/favicon.ico",
  "expired": false,
  "author": {
    "name": "Students from M2 Data Science IP Paris",
    "url": "http://localhost:1313/"
  },
  "items": [
    
    

    
    {
      "id": "5ffb376e110a1e7a4bec65643499306479aa739b",
      "title": "Packed-Ensembles: Efficient Neural Network Ensembles Made Easy",
      "summary": "",
      "content_text": "Introduction:\nNeural networks are powerful tools used in various tasks like image recognition, natural language processing, and time series forecasting. However, training large ensembles of neural networks can be computationally expensive and require significant memory resources. This is where Packed-Ensembles (PE) come in!\nPacked-Ensembles: A Lightweight Ensemble Approach\nPacked-Ensembles (PE) offer a compelling approach to creating lightweight and efficient ensembles of neural networks. This technique leverages the power of grouped convolutions within a single network, enabling the training of multiple subnetworks simultaneously. This blog post dives into the details of Packed-Ensembles, exploring their benefits and how they can improve efficiency in neural network training.\nUnderstanding Convolutional Layers and Grouped Convolutions:\nConvolutional Layers: These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by z_(j+1), is calculated as follows: z_(j+1)(c,:,:) = (h_j ⊗ ω_j)(c,:,:) = ∑_(k=0)^(C_(j-1)-1) ω_j(c, k,:,:) ⋆ h_j(k,:,:) where:\nc represents the channel index h_j denotes the input feature map ω_j represents the weight tensor (kernel) ⋆ denotes the 2D cross-correlation operator Grouped Convolutions: This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating independent subnetworks. The mathematical formulation for grouped convolutions is given by: z_(j+1)(c,:,:) = (h_j ⊗ ω_j^(γ))(c,:,:) = ⌊ c / (C_(j+1) / γ) ⌋ C^(γ)_j,:,:) ⋆ h_j(k + ⌊ c / (C_j / γ) ⌋, :, :) where:\nγ represents the number of groups C_(j+1) and C_j denote the number of output and input channels, respectively Building Packed-Ensembles:\nPacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\nSubnetworks: The ensemble is formed by creating M smaller subnetworks within the main network architecture. These subnetworks share the same structure but have independent parameters due to the use of grouped convolutions. Hyperparameters: Packed-Ensembles are defined by three hyperparameters: α (alpha): expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters). M: number of subnetworks in the ensemble (represents the ensemble size). γ (gamma): number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity). Mathematical Implementation:\nThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\nŷ = M^(-1) Σ_(m=0)^(M-1) P(y|θ_a^m, x) with θ_a^m = {ω_j^(α) ∘ mask_(jm)^j}_j where:\nŷ (y-hat) represents the ensemble\u0026rsquo;s predicted label P(y|θ_a^m, x) denotes the probability of class y given the input x and the parameters θ_a^m of the m-th subnetwork θ_a^m = {ω_j^(α) ∘ mask_(jm)^j}_j represents the parameters of the m-th subnetwork, obtained by applying element-wise multiplication (∘) between the expanded weights (ω_j^(α)) and the group mask (mask_(jm)) for each layer j Benefits of Packed-Ensembles:\nPacked-Ensembles offer several advantages over traditional ensemble methods:\nReduced Memory Usage and Training Time: By utilizing grouped convolutions and sharing parameters between subnetworks, PE significantly reduces memory footprint and training time compared to training individual DNNs in an ensemble. This is because the subnetworks share the same base architecture and only differ in their learnable parameters, leading to a more efficient use of memory resources and computational power.\nAccuracy: PE can achieve accuracy levels comparable to traditional ensembles. This demonstrates their effectiveness in preserving ensemble properties like diversity despite using fewer parameters. This is crucial as reducing the number of parameters in an ensemble can potentially lead to a decrease in accuracy. Packed-Ensembles strike a balance between efficiency and accuracy, making them suitable for various applications where both aspects are important.\nCalibration: Packed-Ensembles are well-calibrated, meaning that their predicted probabilities accurately reflect the true probabilities. This is crucial in tasks where reliable confidence estimates are essential, such as in medical diagnosis or autonomous decision-making systems. Well-calibrated models provide a more accurate understanding of the model\u0026rsquo;s certainty in its predictions, allowing for better decision-making based on the outputs.\nOut-of-distribution (OOD) detection: Packed-Ensembles are good at detecting out-of-distribution (OOD) data. This refers to data that comes from a different distribution than the data that the model was trained on. Detecting OOD data is important for ensuring the model\u0026rsquo;s robustness and preventing it from making unreliable predictions on unseen data. Packed-Ensembles can effectively identify OOD data, contributing to the model\u0026rsquo;s reliability and generalizability.\nComparison to Other Ensemble Methods:\nThe paper comparing Packed-Ensembles to other methods, such as Deep Ensembles, BatchEnsemble, MIMO, and Masksembles, found that PE are:\nMore efficient in terms of memory usage and training time due to their use of grouped convolutions and parameter sharing. Able to achieve comparable accuracy to most of the compared methods, demonstrating their effectiveness in maintaining ensemble properties. Packed-Ensembles: A Promising Technique for Efficient Neural Network Ensembles\nPacked-Ensembles offer a promising approach for creating efficient and accurate neural network ensembles. Their ability to reduce memory usage and training time while maintaining accuracy, calibration, and OOD detection capabilities makes them a valuable tool for various applications in deep learning. As research in this area continues, Packed-Ensembles have the potential to become even more efficient and versatile, further expanding their role in advancing the field of neural networks.\nAdditional Notes:\nYou can consider adding visuals such as diagrams or illustrations to further explain complex concepts like convolutional layers and grouped convolutions.\nInclude a link to the research paper on Packed-Ensembles or relevant resources for readers who want to learn more.\nRead the Full Paper\nAdjust the technical level of the explanations based on your target audience.\nBy incorporating images, you enhance the visual appeal and understanding of the content, making it more accessible and engaging for readers. Additionally, providing links to resources allows interested readers to delve deeper into the subject. Feel free to replace the placeholder images with relevant visuals that align with the content.\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNeural networks are powerful tools used in various tasks like image recognition, natural language processing, and time series forecasting. However, training large ensembles of neural networks can be computationally expensive and require significant memory resources. This is where Packed-Ensembles (PE) come in!\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/static/my_images/ip-logo.png\"\r\n  alt=\"Neural Network\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePacked-Ensembles: A Lightweight Ensemble Approach\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles (PE) offer a compelling approach to creating \u003cstrong\u003elightweight and efficient ensembles\u003c/strong\u003e of neural networks. This technique leverages the power of \u003cstrong\u003egrouped convolutions\u003c/strong\u003e within a single network, enabling the training of \u003cstrong\u003emultiple subnetworks simultaneously\u003c/strong\u003e. This blog post dives into the details of Packed-Ensembles, exploring their benefits and how they can improve efficiency in neural network training.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderstanding Convolutional Layers and Grouped Convolutions:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConvolutional Layers:\u003c/strong\u003e These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by \u003cstrong\u003ez_(j+1)\u003c/strong\u003e, is calculated as follows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-math\" data-lang=\"math\"\u003ez_(j+1)(c,:,:) = (h_j ⊗ ω_j)(c,:,:) = ∑_(k=0)^(C_(j-1)-1) ω_j(c, k,:,:) ⋆ h_j(k,:,:)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ec\u003c/strong\u003e represents the channel index\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eh_j\u003c/strong\u003e denotes the input feature map\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eω_j\u003c/strong\u003e represents the weight tensor (kernel)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e⋆\u003c/strong\u003e denotes the 2D cross-correlation operator\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"convolutional_layer_image.jpg\"\r\n  alt=\"Convolutional Layer\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped Convolutions:\u003c/strong\u003e This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating \u003cstrong\u003eindependent subnetworks\u003c/strong\u003e. The mathematical formulation for grouped convolutions is given by:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-math\" data-lang=\"math\"\u003ez_(j+1)(c,:,:) = (h_j ⊗ ω_j^(γ))(c,:,:) = ⌊ c / (C_(j+1) / γ) ⌋ C^(γ)_j,:,:) ⋆ h_j(k + ⌊ c / (C_j / γ) ⌋, :, :)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eγ\u003c/strong\u003e represents the number of groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC_(j+1)\u003c/strong\u003e and \u003cstrong\u003eC_j\u003c/strong\u003e denote the number of output and input channels, respectively\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"grouped_convolutions_image.jpg\"\r\n  alt=\"Grouped Convolutions\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBuilding Packed-Ensembles:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSubnetworks:\u003c/strong\u003e The ensemble is formed by creating \u003cstrong\u003eM\u003c/strong\u003e smaller subnetworks within the main network architecture. These subnetworks share the same structure but have \u003cstrong\u003eindependent parameters\u003c/strong\u003e due to the use of grouped convolutions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameters:\u003c/strong\u003e Packed-Ensembles are defined by three hyperparameters:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eα (alpha):\u003c/strong\u003e expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eM:\u003c/strong\u003e number of subnetworks in the ensemble (represents the ensemble size).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eγ (gamma):\u003c/strong\u003e number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"packed_ensemble_image.jpg\"\r\n  alt=\"Packed-Ensemble\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical Implementation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-math\" data-lang=\"math\"\u003eŷ = M^(-1) Σ_(m=0)^(M-1) P(y|θ_a^m, x) with θ_a^m = {ω_j^(α) ∘ mask_(jm)^j}_j\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eŷ (y-hat)\u003c/strong\u003e represents the ensemble\u0026rsquo;s predicted label\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eP(y|θ_a^m, x)\u003c/strong\u003e denotes the probability of class \u003cstrong\u003ey\u003c/strong\u003e given the input \u003cstrong\u003ex\u003c/strong\u003e and the parameters \u003cstrong\u003eθ_a^m\u003c/strong\u003e of the \u003cstrong\u003em-th\u003c/strong\u003e subnetwork\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eθ_a^m = {ω_j^(α) ∘ mask_(jm)^j}_j\u003c/strong\u003e represents the parameters of the \u003cstrong\u003em-th\u003c/strong\u003e subnetwork, obtained by applying element-wise multiplication (\u003cstrong\u003e∘\u003c/strong\u003e) between the expanded weights (\u003cstrong\u003eω_j^(α)\u003c/strong\u003e) and the group mask (\u003cstrong\u003emask_(jm)\u003c/strong\u003e) for each layer \u003cstrong\u003ej\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"mathematical_implementation_image.jpg\"\r\n  alt=\"Mathematical Implementation\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBenefits of Packed-Ensembles:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles offer several advantages over traditional ensemble methods:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eReduced Memory Usage and Training Time:\u003c/strong\u003e By utilizing grouped convolutions and sharing parameters between subnetworks, PE significantly reduces memory footprint and training time compared to training individual DNNs in an ensemble. This is because the subnetworks share the same base architecture and only differ in their learnable parameters, leading to a more efficient use of memory resources and computational power.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccuracy:\u003c/strong\u003e PE can achieve accuracy levels comparable to traditional ensembles. This demonstrates their effectiveness in preserving ensemble properties like diversity despite using fewer parameters. This is crucial as reducing the number of parameters in an ensemble can potentially lead to a decrease in accuracy. Packed-Ensembles strike a balance between efficiency and accuracy, making them suitable for various applications where both aspects are important.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalibration:\u003c/strong\u003e Packed-Ensembles are well-calibrated, meaning that their predicted probabilities accurately reflect the true probabilities. This is crucial in tasks where reliable confidence estimates are essential, such as in medical diagnosis or autonomous decision-making systems. Well-calibrated models provide a more accurate understanding of the model\u0026rsquo;s certainty in its predictions, allowing for better decision-making based on the outputs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOut-of-distribution (OOD) detection:\u003c/strong\u003e Packed-Ensembles are good at detecting out-of-distribution (OOD) data. This refers to data that comes from a different distribution than the data that the model was trained on. Detecting OOD data is important for ensuring the model\u0026rsquo;s robustness and preventing it from making unreliable predictions on unseen data. Packed-Ensembles can effectively identify OOD data, contributing to the model\u0026rsquo;s reliability and generalizability.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eComparison to Other Ensemble Methods:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe paper comparing Packed-Ensembles to other methods, such as Deep Ensembles, BatchEnsemble, MIMO, and Masksembles, found that PE are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMore efficient\u003c/strong\u003e in terms of memory usage and training time due to their use of grouped convolutions and parameter sharing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAble to achieve comparable accuracy\u003c/strong\u003e to most of the compared methods, demonstrating their effectiveness in maintaining ensemble properties.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePacked-Ensembles: A Promising Technique for Efficient Neural Network Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles offer a promising approach for creating efficient and accurate neural network ensembles. Their ability to \u003cstrong\u003ereduce memory usage and training time\u003c/strong\u003e while \u003cstrong\u003emaintaining accuracy, calibration, and OOD detection capabilities\u003c/strong\u003e makes them a valuable tool for various applications in deep learning. As research in this area continues, Packed-Ensembles have the potential to become even more efficient and versatile, further expanding their role in advancing the field of neural networks.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAdditional Notes:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou can consider adding visuals such as diagrams or illustrations to further explain complex concepts like convolutional layers and grouped convolutions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"visual_explanation_image.jpg\"\r\n  alt=\"Visual Explanation\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInclude a link to the research paper on Packed-Ensembles or relevant resources for readers who want to learn more.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"link_to_paper.pdf\"\u003eRead the Full Paper\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAdjust the technical level of the explanations based on your target audience.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy incorporating images, you enhance the visual appeal and understanding of the content, making it more accessible and engaging for readers. Additionally, providing links to resources allows interested readers to delve deeper into the subject. Feel free to replace the placeholder images with relevant visuals that align with the content.\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/packed-ensembles/",
      "date_published": "27026-27-09T23:2727:00+01:00",
      "date_modified": "27026-27-09T23:2727:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "aba6ac7516897c4ee7afcdf83296daebaa43622b",
      "title": "Another article",
      "summary": "",
      "content_text": "Authors : John Smith and John Smith\nDo not forget to add the script posted on moodle to enable latex in your blogpost! What a beauty! $y=\\theta_0 + \\theta_1x_1$\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\r\n\u003cstyle\r\nTYPE=\"text/css\"\u003e\r\n\u003cp\u003ecode.has-jax {font:\ninherit;\nfont-size:\n100%;\nbackground:\ninherit;\nborder:\ninherit;}\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript\r\ntype=\"text/x-mathjax-config\"\u003e\r\n\r\nMathJax.Hub.Config({\r\n\r\n    tex2jax: {\r\n\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n\r\n    }\r\n\r\n});\r\n\r\nMathJax.Hub.Queue(function() {\r\n\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n\r\n    }\r\n\r\n});\r\n\r\n\u003c/script\u003e\r\n\u003cscript\r\ntype=\"text/javascript\"\r\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n\u003cp\u003eDo not forget to add the script posted on moodle to enable latex in your blogpost!\nWhat a beauty! $y=\\theta_0 + \\theta_1x_1$\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/my-first-blog/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "94409f4b19cf119747df5e05d9507c6fc3ae2286",
      "title": "Title of the article",
      "summary": "",
      "content_text": "Authors : John Smith and John Smith\nStart writing here !\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\r\n\u003cp\u003eStart writing here !\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/my-second-blog/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    }
    
  ]
}